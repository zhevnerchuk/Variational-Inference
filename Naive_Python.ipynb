{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from math import pi, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI():\n",
    "    '''Class for stochastic variational inference\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, joint_distr, var_distr, opt):\n",
    "        '''Initialization\n",
    "        \n",
    "        Args:\n",
    "            joint_distr: requred methods: joint_logpdf(x, z)\n",
    "            var_distr: required methods: logpdf(z), sample()\n",
    "            opt: optimizer\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.joint_distr = joint_distr\n",
    "        self.var_distr = var_distr\n",
    "        self.opt = opt\n",
    "    \n",
    "    def mc_elbo_(self, num_samples, batch):\n",
    "        '''Computing naive Monte Carlo approximation of ELBO over batch\n",
    "        \n",
    "        Args:\n",
    "            num_samples: number of samples used for approximation\n",
    "            batch: batch of observed variables\n",
    "        \n",
    "        Returns:\n",
    "            mc_elbo: naive Monte Carlo approximation of ELBO over batch\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        mc_elbo = Variable(torch.zeros(1))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            z = self.var_distr.sample()\n",
    "            mc_elbo += self.joint_distr.joint_logpdf(batch, z) - self.var_distr.logpdf(z)\n",
    "                \n",
    "        mc_elbo /= num_samples\n",
    "        \n",
    "        return mc_elbo\n",
    "    \n",
    "    def make_inference(self, num_steps, batch_iterator, num_samples=10, print_progress=False):\n",
    "        '''Making SVI\n",
    "        \n",
    "        Args:\n",
    "            num_steps: number of epoches\n",
    "            batch_iterator: iterator over batches\n",
    "            num_samples: number of samples used for ELBO approximation\n",
    "            print_progress: boolean, if True progress bar is printed\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            batch_elbo = []\n",
    "            for batch in batch_iterator:\n",
    "                loss = self.mc_elbo_(num_samples, batch)\n",
    "                batch_elbo.append(float(loss))\n",
    "                (-loss).backward()\n",
    "                opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            if print_progress:\n",
    "                if (int(25 * step / num_steps) != int(25 * (step - 1) / num_steps)):\n",
    "                    print('.', end='')\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример:**\n",
    "\n",
    "Мы воспроизводим простейший пример вариационного вывода, приведенный в [документации Pyro](http://pyro.ai/examples/svi_part_i.html#A-simple-example)\n",
    "\n",
    "Есть монетка, вероятность выпадения единицы равна $p$. Дан (неупорядоченный) набор результатов подбрасывания монетки. Хотим по нему оценить $p$. Сопряженное к распределению Бернулли -- бета-распределение, поэтому мы будем считать, что априорное распределение на $p$ -- бета-распределение с параметрами (10, 10).\n",
    "\n",
    "В качестве априорного распределения на $p$ мы берем бета-распределение с параметрами 10 и 10. В таком случае апостериорное распределение тоже будет бета-распределением, но с другими параметрами. Потому в качестве вариационного распределения мы также берем бета-распределение.\n",
    "\n",
    "**Пример не работает на стабильной версии PyTorch, т.к. на момент ее релиза еще не был реализован проброс градиента через гамма-функцию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDistribution():\n",
    "    \n",
    "    def __init__(self, prior):\n",
    "        self.parameters = prior.parameters\n",
    "        self.prior = prior\n",
    "    \n",
    "    def cond_logpdf(self, x, z):\n",
    "        return torch.sum(torch.log(z * x + (1 - z) * (1 - x)))\n",
    "    \n",
    "    def joint_logpdf(self, x, z):\n",
    "        return self.prior.logpdf(z) + self.cond_logpdf(x, z)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beta():\n",
    "    \n",
    "    def __init__(self, alpha=1., beta=1.):\n",
    "        self.log_alpha = Parameter(torch.log(torch.FloatTensor([alpha])))\n",
    "        self.log_beta = Parameter(torch.log(torch.FloatTensor([beta])))\n",
    "        self.parameters = [self.log_alpha, self.log_beta]\n",
    "        \n",
    "    def logpdf(self, x):\n",
    "        eps = 1e-8\n",
    "        logbeta = torch.lgamma(torch.exp(torch.clamp(self.log_alpha, -10, 10))) + \\\n",
    "                  torch.lgamma(torch.exp(torch.clamp(self.log_beta, -10, 10))) - \\\n",
    "                  torch.lgamma(torch.exp(torch.clamp(self.log_alpha, -10, 10)) + \\\n",
    "                               torch.exp(torch.clamp(self.log_beta, -10, 10)))\n",
    "        y = torch.clamp(x, eps, 1 - eps)\n",
    "        alpha_term = (torch.exp(torch.clamp(self.log_alpha, -10, 10)) - 1) * torch.log(y)\n",
    "        beta_term = (torch.exp(torch.clamp(self.log_beta, -10, 10)) - 1) * torch.log(1 - y)\n",
    "        return alpha_term + beta_term - logbeta\n",
    "    \n",
    "    def sample(self):\n",
    "        return Variable(torch.FloatTensor(np.random.beta(torch.exp(torch.clamp(self.log_alpha, -10, 10)).data.numpy(),\n",
    "                                                         torch.exp(torch.clamp(self.log_beta, -10, 10)).data.numpy())),\n",
    "                        requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = Beta(15, 15)\n",
    "prior = Beta(10, 10)\n",
    "joint = JointDistribution(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1.]] * 6 + [[0.]] * 4\n",
    "data = Variable(torch.FloatTensor(data))\n",
    "data = data.view(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(var.parameters, lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(joint, var, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................"
     ]
    }
   ],
   "source": [
    "svi.make_inference(4000, data, 25, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.576 +- 0.091\n"
     ]
    }
   ],
   "source": [
    "# grab the learned variational parameters\n",
    "alpha_q = torch.exp(var.log_alpha).data.numpy()[0]\n",
    "beta_q = torch.exp(var.log_beta).data.numpy()[0]\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * np.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = Beta(15, 15)\n",
    "prior = Beta(10, 10)\n",
    "joint = JointDistribution(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0.]] * 9 + [[1.]] * 1\n",
    "data = Variable(torch.FloatTensor(data))\n",
    "data = data.view(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(var.parameters, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(joint, var, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................"
     ]
    }
   ],
   "source": [
    "svi.make_inference(2000, data, 10, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.321 +- 0.081\n"
     ]
    }
   ],
   "source": [
    "# grab the learned variational parameters\n",
    "alpha_q = torch.exp(var.log_alpha).data.numpy()[0]\n",
    "beta_q = torch.exp(var.log_beta).data.numpy()[0]\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * np.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
