{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Example\n",
    "\n",
    "Assume we have mixture of two Gaussians and we want to distinguish them. Here we do it using Black Box SVI II algorithm, described in the article by [Ranganath et al.](https://arxiv.org/abs/1401.0118)\n",
    "\n",
    "Our model contains one local variable $z_i$ for every observed data point $x_i$, which reflects the component of our mixture $x_i$ came from. Both Gaussians have variance equals to one and means $\\mu_1$ and $\\mu_2$ we wants to find.\n",
    "\n",
    "Our variational distribution consists from $N$ independent Categorical Distributions $q_i(z_i | x_i)$, which parameters we want to tune. At the end we assign every data point to one of the mixture components according to this distribution.\n",
    "\n",
    "While the inference we learn both $q(Z | X)$ and $\\mu_1$ and $\\mu_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from math import pi, log\n",
    "from BBSVI import SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating data\n",
    "std = 1\n",
    "mu = np.array([-5, 5])\n",
    "num_components = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "num_samples = 100\n",
    "\n",
    "components = np.random.choice(num_components, size=num_samples, p=np.array([1/2, 1/2]))\n",
    "data = torch.Tensor(np.random.normal(mu[components], std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define two classes for prior and variational distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "    def __init__(self, num_components, std=1):\n",
    "        \n",
    "        self.std = torch.Tensor([std])\n",
    "        self.means = nn.Parameter(1 * torch.randn(num_components, requires_grad=True))\n",
    "        self.weights = torch.ones(num_components, requires_grad=True) / num_components\n",
    "        self.weights_distr = torch.distributions.Categorical(probs=self.weights)\n",
    "        self.components_distrs = [torch.distributions.Normal(self.means[i], \n",
    "                                                             self.std) for i in range(num_components)]\n",
    "        self.parameters = [self.means]\n",
    "\n",
    "    def log_likelihood_global(self, beta):\n",
    "        return torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    def log_likelihood_local(self, z, beta):\n",
    "        return self.weights_distr.log_prob(z)\n",
    "    \n",
    "    def log_likelihood_joint(self, x, z, beta):\n",
    "        mix_term = self.weights_distr.log_prob(z)\n",
    "        normal_term = self.components_distrs[z].log_prob(x)\n",
    "        return mix_term + normal_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalDistribution:\n",
    "    def __init__(self, num_components, data_size):\n",
    "        self.num_components = num_components\n",
    "        self.probs = [torch.nn.Parameter(torch.ones(num_components) / num_components) for _ in range(data_size)]\n",
    "        self.distrs = [torch.distributions.Categorical(probs=self.probs[i]) for i in range(data_size)]\n",
    "        self.global_parameters = []\n",
    "        self.local_parameters = self.probs\n",
    "        \n",
    "    def sample_global(self, num_samples = 1):\n",
    "        return None\n",
    "    \n",
    "    def sample_local(self, beta, idx):\n",
    "        return self.distrs[idx].sample()\n",
    "    \n",
    "    def entropy(self, batch_indices):\n",
    "        ent = torch.zeros(1, requires_grad=True)\n",
    "        for idx in batch_indices:\n",
    "            ent = ent + self.distrs[idx].entropy()\n",
    "        return ent\n",
    "    \n",
    "    def log_likelihood_global(self, beta):\n",
    "        return torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    def log_likelihood_local(self, z, idx):\n",
    "        return self.distrs[idx].log_prob(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prior, variational distribution and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMixture(num_components)\n",
    "var = VariationalDistribution(num_components, num_samples)\n",
    "opt = torch.optim.Adam([{'params': var.local_parameters},\n",
    "                        {'params': var.global_parameters},\n",
    "                        {'params': prior.parameters}], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svi = SVI(data, prior, var, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial means:   \t 1.07 \t -1.26\n",
      "...................................................................................................\n",
      "Predicted means: \t 4.33 \t -4.44\n",
      "Real means:      \t -5.00 \t 5.00\n",
      "Mixture components detecting accuracy: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "predicted_mu = prior.means.data.numpy()\n",
    "print('Initial means:   \\t %.2f \\t %.2f' % (predicted_mu[0], predicted_mu[1]))\n",
    "\n",
    "num_steps = 300\n",
    "discounter_scheduler = torch.Tensor(np.linspace(0, 1, num_steps))\n",
    "svi.make_inference(num_steps=num_steps, shuffle=False, loss='bb2', \n",
    "                   print_progress=True, retain_graph=True, discounter_schedule=discounter_scheduler)\n",
    "\n",
    "predicted_mu = prior.means.data.numpy()\n",
    "print('Predicted means: \\t %.2f \\t %.2f' % (predicted_mu[0], predicted_mu[1]))\n",
    "print('Real means:      \\t %.2f \\t %.2f' % (mu[0], mu[1]))\n",
    "\n",
    "predicted_components = np.array([torch.max(var.probs[i], dim=-1)[1].data.numpy() for i in range(num_samples)])\n",
    "accuracy = np.sum(predicted_components == components) / len(predicted_components)\n",
    "accuracy = max(accuracy, 1 - accuracy)\n",
    "print('Mixture components detecting accuracy: %.2f %%' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** SVI strongly depends on the initialization configuration. If you fail to reproduce good result, try to reinitialize initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
