{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from math import pi, log\n",
    "# from BBSVI import SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SVI():\n",
    "    '''Class for black box stochastic variational inference\n",
    "        https://arxiv.org/abs/1401.0118\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, prior_distr, var_distr, opt, scheduler=None):\n",
    "        '''Initialization\n",
    "        \n",
    "        Args:\n",
    "            data: oserved data\n",
    "            prior_distr: class for prior probabilistic model\n",
    "                requred methods: log_likelihood_global(beta)\n",
    "                                 log_likelihood_local(z, beta)\n",
    "                                 log_likelihood_joint(x, z, beta)\n",
    "            var_distr: class for variational distribution\n",
    "                required methods: log_likelihood_global(beta)\n",
    "                                  log_likelihood_local(z, idx)\n",
    "                                  sample_global()\n",
    "                                  sample_local(idx)\n",
    "            opt: optimizer\n",
    "            scheduler: scheduler for an optimizer\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.data = data\n",
    "        self.prior_distr = prior_distr\n",
    "        self.var_distr = var_distr\n",
    "        self.opt = opt\n",
    "        self.scheduler = scheduler      \n",
    "\n",
    "    def bb1_loss_(self, num_samples, batch_indices):\n",
    "        '''Computing loss of BB SVI 1\n",
    "        \n",
    "        Args:\n",
    "            num_samples: number of samples used for approximation\n",
    "            batch_indices: indices of batch\n",
    "        \n",
    "        Returns:\n",
    "            loss: Black Box loss function\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        global_loss = torch.zeros(1, requires_grad=True)\n",
    "        local_loss = torch.zeros(1, requires_grad=True)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            \n",
    "            beta = self.var_distr.sample_global()\n",
    "            global_const_term = torch.zeros(1, requires_grad=False)\n",
    "            \n",
    "            for idx in batch_indices:\n",
    "                x = self.data[idx]\n",
    "                z = self.var_distr.sample_local(beta, idx)\n",
    "                \n",
    "                local_const_term = self.prior_distr.log_likelihood_local(z, beta) + \\\n",
    "                                   self.prior_distr.log_likelihood_joint(x, z, beta) - \\\n",
    "                                   self.var_distr.log_likelihood_local(z, idx)\n",
    "                \n",
    "                local_var_term = self.var_distr.log_likelihood_local(z, idx)\n",
    "                \n",
    "                local_loss = local_loss + local_var_term * local_const_term.data\n",
    "                \n",
    "                global_const_term += self.prior_distr.log_likelihood_local(z, beta) + \\\n",
    "                                     self.prior_distr.log_likelihood_joint(x, z, beta)\n",
    "            \n",
    "            global_const_term *= self.data.shape[0] / batch_indices.shape[0]\n",
    "            global_const_term += self.prior_distr.log_likelihood_global(beta) - \\\n",
    "                                 self.var_distr.log_likelihood_global(beta)\n",
    "            \n",
    "            global_var_term = self.var_distr.log_likelihood_global(beta)\n",
    "            global_loss = global_loss + global_var_term * global_const_term.data\n",
    "                    \n",
    "        loss = -(global_loss + local_loss) / num_samples\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def bb2_loss_(self, num_samples, batch_indices):\n",
    "        '''Computing loss of BB SVI 2, which has lower variance compare to BB SVI 1\n",
    "        \n",
    "        Args:\n",
    "            num_samples: number of samples used for approximation\n",
    "            batch_indices: indices of batch\n",
    "        \n",
    "        Returns:\n",
    "            loss: Black Box loss function\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        loss_global = torch.zeros(1, requires_grad=True)\n",
    "        loss_local = torch.zeros(1, requires_grad=True)\n",
    "        \n",
    "        global_samples = [self.var_distr.sample_global() for _ in range(num_samples)]\n",
    "        a = torch.autograd.grad(self.var_distr.log_likelihood_global(global_samples[0]), \n",
    "                                self.var_distr.parameters, \n",
    "                                allow_unused=True)\n",
    "        a = [torch.zeros(1) if x is None else x for x in a]\n",
    "        \n",
    "        b = torch.autograd.grad(self.var_distr.log_likelihood_global(global_samples[1]), \n",
    "                                self.var_distr.parameters, \n",
    "                                allow_unused=True)\n",
    "        b = [torch.zeros(1) if x is None else x for x in b]\n",
    "        c = tuple(map(operator.add, a, b))\n",
    "        print(a)\n",
    "        print(b)\n",
    "        print(c)\n",
    "        raise Exception\n",
    "        for idx in batch_indices:\n",
    "            pass\n",
    "\n",
    "    def make_inference(self, num_steps=100, num_samples=10, batch_size=10, shuffle=False, print_progress=True):\n",
    "        '''Making SVI\n",
    "        \n",
    "        Args:\n",
    "            num_steps: int, maximum number of epoches\n",
    "            tol: required tolerance\n",
    "            num_samples: int, number of samples used for ELBO approximation\n",
    "            batch_size: int, size of one batch\n",
    "            shuffle: boolean, if batch is shuffled every epoch or not\n",
    "            print_progress: boolean, if True then progrss bar is printed\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            \n",
    "            if shuffle:\n",
    "                indices = np.random.choice(self.data.shape[0], self.data.shape[0], False)\n",
    "            else:\n",
    "                indices = np.arange(self.data.shape[0])\n",
    "                \n",
    "            indices = np.split(indices, np.arange(batch_size, self.data.shape[0], batch_size))\n",
    "                \n",
    "            for batch_indices in indices:\n",
    "                self.opt.zero_grad()\n",
    "                loss = self.bb1_loss_(num_samples, batch_indices)\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "\n",
    "            if print_progress:\n",
    "                if (int(25 * step / num_steps) != int(25 * (step - 1) / num_steps)):\n",
    "                    print('.', end='')\n",
    "        \n",
    "        if print_progress:\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Example\n",
    "\n",
    "Assume we have mixture of some Gaussians and we want to distinguish them. Here we do it using Black Box SVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating data\n",
    "\n",
    "std = 1\n",
    "mu = np.array([-5, 5])\n",
    "num_components = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "num_samples = 100\n",
    "\n",
    "components = np.random.randint(num_components, size=num_samples)\n",
    "data = torch.Tensor(np.random.normal(mu[components], std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define two classes for prior and variational distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "    def __init__(self, num_components, std=1.):\n",
    "        self.std = std\n",
    "        self.num_components = num_components\n",
    "        self.prior = torch.distributions.Normal(0, std)\n",
    "        \n",
    "    def log_likelihood_global(self, beta):\n",
    "        return torch.sum(self.prior.log_prob(beta))\n",
    "    \n",
    "    def log_likelihood_local(self, z, beta):\n",
    "        return torch.ones(1) / self.num_components\n",
    "    \n",
    "    def log_likelihood_joint(self, x, z, beta):\n",
    "        dist = torch.distributions.Normal(beta[z], 1)\n",
    "        return dist.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalDistribution:\n",
    "    def __init__(self, num_components, data_size):\n",
    "        self.num_components = num_components\n",
    "        self.means = torch.nn.Parameter(torch.normal(torch.zeros(num_components), torch.ones(num_components)))\n",
    "        self.log_std = torch.nn.Parameter(torch.zeros(num_components))\n",
    "        self.probs = torch.nn.Parameter(torch.ones(data_size, num_components) / num_components)\n",
    "        self.parameters = [self.means, self.log_std, self.probs]\n",
    "        \n",
    "    def sample_global(self):\n",
    "        return torch.normal(self.means, torch.exp(self.log_std))\n",
    "    \n",
    "    def sample_local(self, beta, idx):\n",
    "        probs = self.probs[idx].data.numpy()\n",
    "        z = np.random.choice(self.num_components, p=probs / np.sum(probs))\n",
    "        return torch.LongTensor([z])\n",
    "    \n",
    "    def log_likelihood_global(self, beta):\n",
    "        return torch.sum((beta - self.means) ** 2 / (2 * torch.exp(2 * self.log_std)) - self.log_std - log(2 * pi) / 2)\n",
    "    \n",
    "    def log_likelihood_local(self, z, idx):\n",
    "        return self.probs[idx, z] / torch.sum(self.probs[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prior, variational distribution and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = GaussianMixture(num_components, std=10)\n",
    "var = VariationalDistribution(num_components, num_samples)\n",
    "\n",
    "opt = torch.optim.Adam(var.parameters, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svi = SVI(data, prior, var, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial means:   \t 0.24 \t 0.51\n",
      "[tensor([ 2.4586, -0.0703]), tensor([-7.0448, -1.0049]), tensor([ 0.])]\n",
      "[tensor([ 0.1047,  1.0934]), tensor([-1.0110, -2.1955]), tensor([ 0.])]\n",
      "(tensor([ 2.5633,  1.0231]), tensor([-8.0557, -3.2005]), tensor([ 0.]))\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-8fa9fe631086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredicted_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initial means:   \\t %.2f \\t %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredicted_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-377b3d1e9b83>\u001b[0m in \u001b[0;36mmake_inference\u001b[0;34m(self, num_steps, num_samples, batch_size, shuffle, print_progress)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbb2_loss_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-377b3d1e9b83>\u001b[0m in \u001b[0;36mbb2_loss_\u001b[0;34m(self, num_samples, batch_indices)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predicted_mu = var.means.data.numpy()\n",
    "print('Initial means:   \\t %.2f \\t %.2f' % (predicted_mu[0], predicted_mu[1]))\n",
    "svi.make_inference(num_steps=1000, shuffle=False, print_progress=True)\n",
    "\n",
    "predicted_mu = var.means.data.numpy()\n",
    "print('Predicted means: \\t %.2f \\t %.2f' % (predicted_mu[0], predicted_mu[1]))\n",
    "print('Real means:      \\t %.2f \\t %.2f' % (mu[0], mu[1]))\n",
    "\n",
    "predicted_components = torch.max(var.probs, dim=1)[1].data.numpy()\n",
    "accuracy = np.sum(predicted_components == components) / len(predicted_components)\n",
    "accuracy = max(accuracy, 1 - accuracy)\n",
    "print('Mixture components detecting accuracy: %.2f %%' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** SVI strongly depends on the initialization configuration. If you fail to reproduce good result, try to reinitialize initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (0, 1, None) \n",
    "b = (2, 1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7f2abf4762d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-7f2abf4762d7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "[sum(x) for x in zip(a, b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-faab56a851b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "tuple(map(operator.add, a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-a507d222c0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "torch.Tensor([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
